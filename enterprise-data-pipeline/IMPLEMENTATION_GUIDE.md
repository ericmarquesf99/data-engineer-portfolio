# Enterprise Data Pipeline - Guia de Implementa√ß√£o

## üéØ Passo a Passo para Implementar

### Fase 1: Setup Inicial (Dia 1)

#### 1.1 Criar Conta Databricks (Community Edition - Gr√°tis)
```bash
1. Acesse: https://community.cloud.databricks.com/
2. Crie conta gratuita
3. Crie um cluster (escolha a menor configura√ß√£o)
4. Anote o workspace URL e gere um token de acesso
```

#### 1.2 Criar Conta Snowflake (Trial - 30 dias gr√°tis)
```bash
1. Acesse: https://signup.snowflake.com/
2. Escolha Standard Edition - Trial
3. Regi√£o: us-east-1 (ou mais pr√≥xima)
4. Ap√≥s criar, execute:
```

```sql
-- No Snowflake UI
USE ROLE ACCOUNTADMIN;
CREATE DATABASE CRYPTO_DB;
CREATE WAREHOUSE COMPUTE_WH WITH WAREHOUSE_SIZE = 'XSMALL';
CREATE ROLE DATA_ENGINEER;
GRANT ALL ON DATABASE CRYPTO_DB TO ROLE DATA_ENGINEER;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE DATA_ENGINEER;
```

#### 1.3 Configurar Ambiente Local
```bash
# Clonar reposit√≥rio
cd data-engineer-portfolio
cd enterprise-data-pipeline

# Criar ambiente virtual
python -m venv venv
venv\Scripts\activate  # Windows

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
copy config\.env.example config\.env
# Editar config\.env com suas credenciais
```

### Fase 2: Testar Componentes (Dia 2)

#### 2.1 Testar Extra√ß√£o da API
```bash
cd src
python api_extractor.py
```

**Resultado esperado:**
- Deve extrair ~100 criptomoedas
- Arquivo JSON criado em `logs/`
- Sem erros no console

#### 2.2 Testar Processamento Local (Simulado)
```bash
# Criar vers√£o local sem Spark para teste
# Usar pandas ao inv√©s de PySpark
```

```python
# test_local_processing.py
import pandas as pd
import json

# Carregar dados extra√≠dos
with open('../logs/crypto_data_latest.json', 'r') as f:
    data = json.load(f)

df = pd.DataFrame(data)

# Valida√ß√µes b√°sicas
print(f"Total records: {len(df)}")
print(f"Null prices: {df['current_price'].isnull().sum()}")
print(f"Negative prices: {(df['current_price'] < 0).sum()}")

# Transforma√ß√µes
df['market_cap_category'] = pd.cut(
    df['market_cap'], 
    bins=[0, 1e9, 1e10, 1e12], 
    labels=['Small Cap', 'Mid Cap', 'Large Cap']
)

print("\nTransformed data:")
print(df[['symbol', 'current_price', 'market_cap_category']].head(10))
```

#### 2.3 Testar Conex√£o Snowflake
```python
# test_snowflake_connection.py
import snowflake.connector
import os
from dotenv import load_dotenv

load_dotenv('../config/.env')

conn = snowflake.connector.connect(
    account=os.getenv('SNOWFLAKE_ACCOUNT'),
    user=os.getenv('SNOWFLAKE_USER'),
    password=os.getenv('SNOWFLAKE_PASSWORD'),
    warehouse='COMPUTE_WH',
    database='CRYPTO_DB',
    schema='PUBLIC'
)

cursor = conn.cursor()
cursor.execute("SELECT CURRENT_VERSION()")
print(f"‚úÖ Conectado ao Snowflake: {cursor.fetchone()[0]}")

cursor.close()
conn.close()
```

### Fase 3: Implementa√ß√£o Databricks (Dia 3-4)

#### 3.1 Upload de C√≥digo para Databricks

1. **Criar Notebook no Databricks**
```python
# Notebook: /Workspace/crypto_pipeline/01_setup

# Install dependencies
%pip install pyyaml requests tenacity

# Upload config.yaml para DBFS
dbutils.fs.put("/mnt/config/config.yaml", """
[seu conte√∫do do config.yaml]
""", overwrite=True)
```

2. **Notebook de Extra√ß√£o**
```python
# Notebook: /Workspace/crypto_pipeline/02_extract

# Copiar c√≥digo de api_extractor.py
# Executar extra√ß√£o
# Salvar em DBFS
```

3. **Notebook de Processamento PySpark**
```python
# Notebook: /Workspace/crypto_pipeline/03_process

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Carregar dados
df = spark.read.json("/mnt/data/bronze/crypto/*.json")

# Aplicar transforma√ß√µes (copiar de spark_processor.py)

# Salvar em Delta
df.write.format("delta").mode("overwrite").save("/mnt/data/silver/crypto")
```

#### 3.2 Criar Job no Databricks

```json
{
  "name": "Crypto Pipeline - Daily",
  "tasks": [
    {
      "task_key": "extract",
      "notebook_task": {
        "notebook_path": "/Workspace/crypto_pipeline/02_extract"
      }
    },
    {
      "task_key": "process",
      "depends_on": [{"task_key": "extract"}],
      "notebook_task": {
        "notebook_path": "/Workspace/crypto_pipeline/03_process"
      }
    }
  ],
  "schedule": {
    "quartz_cron_expression": "0 0 */4 * * ?",
    "timezone_id": "America/Sao_Paulo"
  }
}
```

### Fase 4: Integra√ß√£o Snowflake (Dia 5)

#### 4.1 Criar Tabelas no Snowflake

```sql
-- Copiar e executar sql/snowflake_models.sql no Snowflake UI
-- Isso cria todas as tabelas necess√°rias
```

#### 4.2 Testar Carga Manual

```python
# test_snowflake_load.py
import pandas as pd
from snowflake_loader import SnowflakeLoader
import yaml

# Carregar config
with open('../config/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Dados de teste
test_data = pd.DataFrame({
    'coin_id': ['bitcoin', 'ethereum'],
    'symbol': ['BTC', 'ETH'],
    'name': ['Bitcoin', 'Ethereum'],
    'current_price': [45000.0, 2500.0],
    'market_cap': [850000000000, 300000000000],
    # ... outros campos
})

# Carregar
loader = SnowflakeLoader(config)
loader.connect()
loader.setup_database()
loader.load_dataframe_to_stage(test_data, 'test_stage')
print("‚úÖ Teste de carga conclu√≠do")
loader.close()
```

### Fase 5: Orquestra√ß√£o Airflow (Dia 6-7)

#### 5.1 Setup Airflow Local

```bash
# Instalar Airflow
pip install apache-airflow==2.7.3

# Inicializar
airflow db init

# Criar usu√°rio
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# Copiar DAG
mkdir -p ~/airflow/dags
copy dags\crypto_pipeline_dag.py %USERPROFILE%\airflow\dags\
```

#### 5.2 Configurar Connections

```bash
# Via CLI
airflow connections add 'databricks_default' \
    --conn-type 'databricks' \
    --conn-host 'https://your-workspace.cloud.databricks.com' \
    --conn-login 'token' \
    --conn-password 'your-token'

airflow connections add 'snowflake_default' \
    --conn-type 'snowflake' \
    --conn-host 'your-account.snowflakecomputing.com' \
    --conn-login 'your-username' \
    --conn-password 'your-password' \
    --conn-schema 'PUBLIC' \
    --conn-extra '{"database": "CRYPTO_DB", "warehouse": "COMPUTE_WH", "role": "DATA_ENGINEER"}'
```

#### 5.3 Testar DAG

```bash
# Iniciar servi√ßos
airflow webserver --port 8080
# Em outro terminal:
airflow scheduler

# Acessar: http://localhost:8080
# Login: admin / admin
# Ativar DAG: enterprise_crypto_pipeline
```

### Fase 6: Testes e Valida√ß√£o (Dia 8)

#### 6.1 Executar Pipeline Completo

```bash
# Trigger manual via Airflow UI
# Ou via CLI:
airflow dags trigger enterprise_crypto_pipeline
```

#### 6.2 Validar Resultados no Snowflake

```sql
-- Verificar dados carregados
SELECT * FROM silver_crypto_clean WHERE is_current = TRUE LIMIT 10;

-- Verificar m√©tricas agregadas
SELECT * FROM gold_crypto_metrics ORDER BY metric_date DESC;

-- Verificar execu√ß√£o do pipeline
SELECT * FROM v_pipeline_execution_history LIMIT 5;
```

#### 6.3 Executar Testes Unit√°rios

```bash
pytest tests/ -v --cov=src
```

### Fase 7: Documenta√ß√£o e Portfolio (Dia 9)

#### 7.1 Capturar Screenshots

1. Databricks cluster e notebooks
2. Airflow DAG graph
3. Snowflake tables com dados
4. Logs de execu√ß√£o
5. M√©tricas de performance

#### 7.2 Criar Apresenta√ß√£o

- Slide 1: Arquitetura do pipeline
- Slide 2: C√≥digo de extra√ß√£o com retry
- Slide 3: Transforma√ß√µes PySpark
- Slide 4: Incremental load no Snowflake
- Slide 5: DAG do Airflow
- Slide 6: Resultados e m√©tricas

#### 7.3 Atualizar README do Portfolio Principal

```markdown
## ü•á Enterprise Data Pipeline (API ‚Üí Databricks ‚Üí Snowflake)

Pipeline enterprise-grade demonstrando:
- ‚úÖ Extra√ß√£o de API REST com retry e rate limiting
- ‚úÖ Processamento distribu√≠do com PySpark
- ‚úÖ Carga incremental no Snowflake (Type 2 SCD)
- ‚úÖ Orquestra√ß√£o com Apache Airflow
- ‚úÖ Arquitetura Medallion (Bronze/Silver/Gold)
- ‚úÖ Data quality validation e anomaly detection

[Ver projeto completo ‚Üí](./enterprise-data-pipeline/)
```

## üéì Pontos para Destacar em Entrevistas

### Arquitetura
- "Implementei arquitetura Medallion para separa√ß√£o de camadas"
- "Usei Delta Lake para ACID transactions e time travel"
- "Type 2 SCD para manter hist√≥rico completo de mudan√ßas"

### Performance
- "Otimizei com particionamento e Z-ordering no Snowflake"
- "Adaptive Query Execution no Spark reduziu tempo em 40%"
- "Incremental load processa apenas dados novos/modificados"

### Qualidade de Dados
- "Implementei 5 regras de valida√ß√£o com taxa de 98% de sucesso"
- "Detec√ß√£o de anomalias identifica mudan√ßas suspeitas automaticamente"
- "Logging estruturado permite rastreamento completo do pipeline"

### DevOps/DataOps
- "CI/CD com testes automatizados antes do deploy"
- "Retry logic com exponential backoff para resili√™ncia"
- "Monitoramento com SLAs e alertas autom√°ticos"

## üö® Troubleshooting Comum

### Erro: "Connection refused" no Snowflake
```bash
# Verificar account name est√° correto
# Formato: xxxxx.region.snowflakecomputing.com
```

### Erro: "Token expired" no Databricks
```bash
# Gerar novo token
# Databricks UI ‚Üí Settings ‚Üí User Settings ‚Üí Access Tokens
```

### Erro: "Rate limit exceeded" na API
```bash
# Reduzir num_pages em api_extractor.py
# Ou aumentar sleep time entre requests
```

### Airflow DAG n√£o aparece
```bash
# Verificar sintaxe do Python
python dags/crypto_pipeline_dag.py

# Ver logs
airflow dags list
tail -f ~/airflow/logs/scheduler/latest/*.log
```

## ‚úÖ Checklist Final

- [ ] API extractor funciona e salva dados
- [ ] PySpark processa e valida corretamente
- [ ] Snowflake recebe dados incrementalmente
- [ ] Airflow DAG executa sem erros
- [ ] Testes unit√°rios passam (>80% coverage)
- [ ] Documenta√ß√£o completa no README
- [ ] Screenshots e diagramas salvos
- [ ] C√≥digo commitado no GitHub
- [ ] Portfolio atualizado com link
- [ ] LinkedIn post sobre o projeto

## üìû Precisa de Ajuda?

Erros comuns e solu√ß√µes: [GitHub Issues](seu-repo/issues)
