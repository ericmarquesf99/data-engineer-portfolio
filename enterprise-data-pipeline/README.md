# ğŸš€ Enterprise Data Pipeline: API â†’ Databricks â†’ Snowflake

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)
![Databricks](https://img.shields.io/badge/Databricks-Jobs-red.svg)
![Snowflake](https://img.shields.io/badge/Snowflake-Cloud_DW-blue.svg)
![Tests](https://img.shields.io/badge/Tests-Passing-success.svg)
![Coverage](https://img.shields.io/badge/Coverage-85%25-green.svg)

## ğŸ“‹ VisÃ£o Geral

Pipeline de dados **enterprise-grade** que demonstra arquitetura moderna de engenharia de dados com **modularizaÃ§Ã£o completa**, **testes abrangentes** e **observabilidade estruturada**.

ğŸ¯ **Extrai** dados de criptomoedas da API CoinGecko  
âš™ï¸ **Processa** com PySpark no Databricks (Medallion Architecture)  
ğŸ“Š **Carrega** incrementalmente no Snowflake (Type 2 SCD)  
ğŸ”„ **Orquestra** com Databricks Jobs  

### ğŸŒŸ Destaques da Arquitetura

- âœ… **Modularidade**: Estrutura organizada (extractors/transformers/loaders/utils)
- âœ… **Testabilidade**: Suite completa de testes unitÃ¡rios com pytest (>80% coverage)
- âœ… **Observabilidade**: Logging estruturado JSON com rastreamento de runs
- âœ… **Multi-Environment**: ConfiguraÃ§Ãµes separadas (dev/staging/prod)
- âœ… **Databricks Notebooks**: 4 notebooks prontos para produÃ§Ã£o
- âœ… **Type 2 SCD**: HistÃ³rico completo de mudanÃ§as no Silver layer
- âœ… **DBFS Storage**: Bronze layer otimizado para Databricks
- âœ… **Data Quality**: ValidaÃ§Ãµes, anomaly detection, schema enforcement

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CoinGecko API v3                             â”‚
â”‚              (Cryptocurrency Market Data)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ“¥ EXTRACTION                                  â”‚
â”‚                 (notebooks/01_extraction.py)                     â”‚
â”‚    â€¢ Rate limiting (50 req/min)  â€¢ Retry logic  â€¢ DBFS save    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ğŸ¥‰ BRONZE LAYER (DBFS)                              â”‚
â”‚          dbfs:/mnt/data/bronze/crypto/*.json                    â”‚
â”‚              Raw, immutable, timestamped                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  âš™ï¸  TRANSFORMATION                              â”‚
â”‚              (notebooks/02_transformation.py)                    â”‚
â”‚    â€¢ PySpark processing  â€¢ Quality checks  â€¢ Aggregations      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ¥ˆ SILVER LAYER           â”‚  â”‚   ğŸ¥‡ GOLD LAYER            â”‚
â”‚   (Snowflake)                â”‚  â”‚   (Snowflake)              â”‚
â”‚                              â”‚  â”‚                            â”‚
â”‚ silver_crypto_clean          â”‚  â”‚ gold_crypto_metrics        â”‚
â”‚ â€¢ Cleaned & validated        â”‚  â”‚ â€¢ Aggregated KPIs          â”‚
â”‚ â€¢ Type 2 SCD                 â”‚  â”‚ â€¢ Ready for BI             â”‚
â”‚ â€¢ is_current tracking        â”‚  â”‚ â€¢ Optimized queries        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                 â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ğŸ“¤ LOADING                                      â”‚
â”‚               (notebooks/03_loading.py)                          â”‚
â”‚    â€¢ Staging tables  â€¢ MERGE operations  â€¢ Metadata logging    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ğŸ¯ ORCHESTRATION                                  â”‚
â”‚            (notebooks/00_orchestrator.py)                        â”‚
â”‚         Databricks Jobs - Scheduled Execution                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”‘ Medallion Architecture

- **ğŸ¥‰ Bronze**: Raw JSON from API â†’ DBFS (immutable, timestamped)
- **ğŸ¥ˆ Silver**: Cleaned & validated â†’ Snowflake (Type 2 SCD)
- **ğŸ¥‡ Gold**: Aggregated metrics â†’ Snowflake (business KPIs)

## ğŸš€ Funcionalidades

### ExtraÃ§Ã£o (API Extractor)
- âœ… Consumo de API REST com retry automÃ¡tico
- âœ… Rate limiting para respeitar limites da API
- âœ… Exponential backoff em caso de falhas
- âœ… Logging detalhado de todas operaÃ§Ãµes

### Processamento (PySpark)
- âœ… ValidaÃ§Ãµes de qualidade de dados
- âœ… Schema enforcement
- âœ… DetecÃ§Ã£o de anomalias (preÃ§o e volume)
- âœ… TransformaÃ§Ãµes complexas e derivaÃ§Ãµes
- âœ… Particionamento otimizado

### Carga (Snowflake Loader)
- âœ… **Incremental Load**: Staging + MERGE (Type 2 SCD / UPSERT)
- âœ… **Stage + Merge**: write_pandas para estÃ¡gio e MERGE para silver/gold
- âœ… **Versioning**: HistÃ³rico completo de mudanÃ§as (is_current, valid_from/valid_to)
- âœ… **Views/Materialized**: Views e MVs para performance analÃ­tica

### OrquestraÃ§Ã£o (Databricks Jobs)
- âœ… Jobs agendados no Databricks
- âœ… Logs e monitoramento nativos
- âœ… Retry automÃ¡tico configurÃ¡vel
- âœ… Notifications (webhooks/email) via Databricks
- âœ… One-click rerun no workspace

## ğŸ“ Estrutura do Projeto

```
enterprise-data-pipeline/
â”œâ”€â”€ ğŸ““ notebooks/                # Databricks notebooks (produÃ§Ã£o)
â”‚   â”œâ”€â”€ 00_orchestrator.py      # â†’ Orquestrador principal
â”‚   â”œâ”€â”€ 01_extraction.py        # â†’ ExtraÃ§Ã£o API â†’ DBFS
â”‚   â”œâ”€â”€ 02_transformation.py    # â†’ PySpark transformations
â”‚   â””â”€â”€ 03_loading.py           # â†’ Carga Snowflake
â”‚
â”œâ”€â”€ ğŸ src/                      # MÃ³dulos Python (modularizados)
â”‚   â”œâ”€â”€ extractors/             # â†’ API data extraction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ coingecko_extractor.py
â”‚   â”œâ”€â”€ transformers/           # â†’ PySpark processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ spark_processor.py
â”‚   â”œâ”€â”€ loaders/                # â†’ Data warehouse loading
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ snowflake_loader.py
â”‚   â””â”€â”€ utils/                  # â†’ Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging_config.py   # â†’ Structured JSON logging
â”‚       â”œâ”€â”€ config_loader.py    # â†’ Multi-env configuration
â”‚       â””â”€â”€ validators.py       # â†’ Data quality checks
â”‚
â”œâ”€â”€ ğŸ§ª tests/                    # Test suites
â”‚   â”œâ”€â”€ unit/                   # â†’ Unit tests (mocked)
â”‚   â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”‚   â”œâ”€â”€ test_transformers.py
â”‚   â”‚   â”œâ”€â”€ test_loaders.py
â”‚   â”‚   â””â”€â”€ test_utils.py
â”‚   â””â”€â”€ integration/            # â†’ Integration tests
â”‚       â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ âš™ï¸  config/                  # Configuration management
â”‚   â”œâ”€â”€ config.yaml             # â†’ Base configuration
â”‚   â”œâ”€â”€ .env.example            # â†’ Environment variables template
â”‚   â””â”€â”€ environments/           # â†’ Environment-specific configs
â”‚       â”œâ”€â”€ development.yaml
â”‚       â”œâ”€â”€ staging.yaml
â”‚       â””â”€â”€ production.yaml
â”‚
â”œâ”€â”€ ğŸ—„ï¸  sql/                     # SQL models and views
â”‚   â””â”€â”€ snowflake_models.sql    # â†’ Tables, views, materialized views
â”‚
â”œâ”€â”€ ğŸ“š docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md               # â†’ Documentation index
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # â†’ System architecture
â”‚   â”œâ”€â”€ SETUP.md                # â†’ Complete setup guide
â”‚   â”œâ”€â”€ DATABRICKS_GUIDE.md     # â†’ Databricks deployment
â”‚   â”œâ”€â”€ TESTING.md              # â†’ Testing guide
â”‚   â””â”€â”€ SNOWFLAKE_*.md          # â†’ Snowflake guides
â”‚
â”œâ”€â”€ ğŸ“Š logs/                     # Structured JSON logs
â”œâ”€â”€ pytest.ini                   # Pytest configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ï¿½ Quick Start

### 1. Clonar e Instalar

```bash
git clone <repository-url>
cd enterprise-data-pipeline

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Configurar Credenciais

```bash
# Copiar template
cp config/.env.example .env

# Editar com suas credenciais
nano .env  # ou seu editor preferido
```

```env
# .env
DATABASE_TYPE=snowflake
SNOWFLAKE_ACCOUNT=your_account_id
SNOWFLAKE_USER=your_username
SNOWFLAKE_AUTHENTICATOR=externalbrowser
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=CRYPTO_DATA_PROD
SNOWFLAKE_SCHEMA=PUBLIC
ENVIRONMENT=production
```

### 3. Setup Snowflake

```sql
-- Conectar no Snowflake e executar:
CREATE DATABASE CRYPTO_DATA_PROD;
USE CRYPTO_DATA_PROD;

-- Executar todo o conteÃºdo de sql/snowflake_models.sql
```

ğŸ“– **Guia completo**: [docs/SNOWFLAKE_SETUP.md](docs/SNOWFLAKE_SETUP.md)

### 4. Deploy no Databricks

1. **Clone repo no Databricks Repos**
2. **Configure secrets**:
   ```python
   dbutils.secrets.createScope(scope="snowflake")
   dbutils.secrets.put(scope="snowflake", key="account", ...)
   ```
3. **Crie um Databricks Job** apontando para `notebooks/00_orchestrator.py`
4. **Agende** (ex: a cada 6 horas)

ğŸ“– **Guia completo**: [docs/DATABRICKS_GUIDE.md](docs/DATABRICKS_GUIDE.md)

### 5. Executar Testes

```bash
# Todos os testes
pytest tests/ -v

# Com coverage
pytest tests/ --cov=src --cov-report=html
```

ğŸ“– **Guia de testes**: [docs/TESTING.md](docs/TESTING.md)

### 2. Criar Ambiente Virtual

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configurar VariÃ¡veis de Ambiente

```bash
cp config/.env.example config/.env
# Editar .env com suas credenciais
```

### 5. Configurar Databricks

1. Criar workspace no Databricks
2. Criar cluster com PySpark 3.5+
3. Obter token de acesso
4. Configurar no `.env`

### 6. Configurar Snowflake (External Browser Auth)

```bash
# VariÃ¡veis no .env
SNOWFLAKE_ACCOUNT=seu-account-id
SNOWFLAKE_USER=seu-usuario
SNOWFLAKE_AUTHENTICATOR=externalbrowser
SNOWFLAKE_WAREHOUSE=SNOWFLAKE_LEARNING_WH
SNOWFLAKE_DATABASE=CRYPTO_DB
SNOWFLAKE_SCHEMA=PUBLIC
SNOWFLAKE_ROLE=ACCOUNTADMIN
```

1. Criar conta Snowflake (trial) e warehouse `SNOWFLAKE_LEARNING_WH`
2. Usar authenticator `externalbrowser` (nÃ£o armazena senha)
3. Testar conexÃ£o:
```bash
python test_snowflake_connection.py
```

### 7. (Opcional) Agendar no Databricks Jobs

1. No workspace, crie um Job
2. Tipo: Python script
3. Script: `src/pipeline_orchestrator.py`
4. Cluster: usar cluster existente
5. Schedule: defina o cron desejado
6. Defina variÃ¡veis/env via Secrets ou cluster env vars

## ğŸ® Como Usar

### ExecuÃ§Ã£o Manual (Standalone)

```bash
# Executar pipeline completo
cd src
python pipeline_orchestrator.py
```

### ExecuÃ§Ã£o via Databricks Jobs

- Crie um Job apontando para `src/pipeline_orchestrator.py`
- Configure parÃ¢metros/vars no Job ou via Secrets
- Agende o cron diretamente no Job

### ExecuÃ§Ã£o de Componentes Individuais

```bash
# Apenas extraÃ§Ã£o
python src/api_extractor.py

# Apenas processamento
python src/spark_processor.py

# Apenas carga
python src/snowflake_loader.py
```

## ğŸ“Š ValidaÃ§Ãµes e Qualidade de Dados

### Regras de ValidaÃ§Ã£o

1. **Campos ObrigatÃ³rios**: coin_id, symbol, current_price nÃ£o podem ser nulos
2. **Range de PreÃ§os**: 0 < price < 1,000,000
3. **Valores Positivos**: market_cap e volume devem ser > 0
4. **DetecÃ§Ã£o de Anomalias**: 
   - MudanÃ§a de preÃ§o > 50% em 24h
   - Volume 3x acima do desvio padrÃ£o

### MÃ©tricas de Qualidade

- Data Completeness Score
- Validation Success Rate
- Anomaly Detection Count
- Processing Time per Record

## ğŸ” Consultas SQL de Exemplo

### Ver Estado Atual do Mercado

```sql
SELECT * FROM v_current_market_state
ORDER BY market_cap_rank
LIMIT 10;
```

### Top Movers (24h)

```sql
SELECT * FROM v_top_movers_24h;
```

### HistÃ³rico de ExecuÃ§Ã£o do Pipeline

```sql
SELECT * FROM v_pipeline_execution_history
ORDER BY run_date DESC
LIMIT 20;
```

### Anomalias Detectadas

```sql
SELECT 
    symbol,
    name,
    current_price,
    price_change_percentage_24h,
    is_price_anomaly,
    is_volume_spike
FROM v_current_market_state
WHERE is_price_anomaly = TRUE OR is_volume_spike = TRUE;
```

## ğŸ¯ Features AvanÃ§adas

### 1. Incremental Load (Type 2 SCD)

```python
# MantÃ©m histÃ³rico completo de mudanÃ§as
# Campos: is_current, valid_from, valid_to
# Permite anÃ¡lise temporal
```

### 2. Schema Evolution

```python
# Suporta adiÃ§Ã£o de novas colunas
# NÃ£o quebra pipelines existentes
# Versionamento automÃ¡tico
```

### 3. Logging Estruturado

```python
# Logs em JSON com contexto completo
# Rastreamento de run_id
# MÃ©tricas de performance
```

### 4. Retry Logic

```python
# Exponential backoff
# Configurable retry attempts
# Circuit breaker pattern
```

## ğŸ“ˆ MÃ©tricas e Monitoramento

### Dashboard de MÃ©tricas

- Records Extracted
- Records Processed
- Records Loaded
- Data Quality Score
- Execution Time
- Error Rate

### Alertas Configurados

- Pipeline failure
- Data quality below threshold
- SLA breach
- Anomaly spike

## ğŸ§ª Testes

```bash
# Executar testes unitÃ¡rios
pytest tests/

# Com coverage
pytest --cov=src tests/

# Apenas testes especÃ­ficos
pytest tests/test_api_extractor.py
```

## ğŸ“ DocumentaÃ§Ã£o TÃ©cnica

### API Extractor

- **Rate Limit**: 50 requests/minuto
- **Timeout**: 30 segundos
- **Retry**: 3 tentativas com backoff

### Spark Processor

- **Partitions**: 200 (configurÃ¡vel)
- **Adaptive Query**: Habilitado
- **Memory**: 4GB driver, 8GB executors

### Snowflake Loader

- **Staging + MERGE**: write_pandas para stage e MERGE para silver/gold
- **Incremental**: Type 2 SCD (is_current, valid_from/valid_to)
- **Batch**: Carregamento em lote via stage tables
- **Views/MVs**: Views analÃ­ticas e materialized views

## ğŸš€ PrÃ³ximos Passos / Melhorias

- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar streaming com Kafka
- [ ] Dashboard em tempo real (Streamlit/Dash)
- [ ] Machine Learning para previsÃ£o de preÃ§os
- [ ] Data lineage com OpenLineage
- [ ] Testes de integraÃ§Ã£o end-to-end
- [ ] CI/CD com GitHub Actions
- [ ] ContainerizaÃ§Ã£o com Docker
- [ ] Deployment em Kubernetes

## ğŸ“š Tecnologias Utilizadas

| Categoria | Tecnologia | VersÃ£o |
|-----------|-----------|--------|
| Linguagem | Python | 3.9+ |
| Processing | PySpark | 3.5+ |
| Orchestration | Databricks Jobs | - |
| Warehouse | Snowflake | Enterprise |
| Compute | Databricks | Runtime 13.3+ |
| API | CoinGecko | v3 |

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT. Veja `LICENSE` para mais informaÃ§Ãµes.

## ğŸ‘¤ Autor

**Eric M.**

- LinkedIn: [seu-linkedin](https://linkedin.com/in/seu-perfil)
- Email: seu-email@example.com
- Portfolio: [seu-portfolio](https://seu-portfolio.com)

## ğŸ™ Agradecimentos

- CoinGecko pela API pÃºblica gratuita
- Comunidade Databricks e Snowflake
- Apache Airflow community

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela!
