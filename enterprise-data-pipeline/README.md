# Enterprise Data Pipeline: API â†’ Databricks â†’ Snowflake

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)
![Databricks Jobs](https://img.shields.io/badge/Orchestration-Databricks%20Jobs-red.svg)
![Snowflake](https://img.shields.io/badge/Snowflake-Enterprise-blue.svg)

## ğŸ“‹ VisÃ£o Geral

Pipeline de dados enterprise-grade que demonstra arquitetura moderna de engenharia de dados, consumindo dados de criptomoedas da API CoinGecko, processando com PySpark no Databricks, e carregando incrementalmente no Snowflake.

### ğŸ¯ Objetivos do Projeto

- âœ… **ETL Real**: Pipeline completo de extraÃ§Ã£o, transformaÃ§Ã£o e carga
- âœ… **PySpark**: Processamento distribuÃ­do e otimizado
- âœ… **SQL Modeling**: Modelagem dimensional e views analÃ­ticas
- âœ… **Cloud Thinking**: Arquitetura escalÃ¡vel e cloud-native
- âœ… **Performance**: OtimizaÃ§Ãµes e melhores prÃ¡ticas

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CoinGecko  â”‚â”€â”€â”€â”€â–¶â”‚  Databricks  â”‚â”€â”€â”€â”€â–¶â”‚  Snowflake   â”‚
â”‚     API     â”‚     â”‚   (PySpark)  â”‚     â”‚ (Warehouse) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚                     â”‚
    â”‚                    â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚ Databricks  â”‚
              â”‚    Jobs     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Medallion Architecture

- **Bronze Layer**: Dados brutos da API (schema-on-read)
- **Silver Layer**: Dados limpos e validados com regras de negÃ³cio
- **Gold Layer**: MÃ©tricas agregadas para anÃ¡lise

## ğŸš€ Funcionalidades

### ExtraÃ§Ã£o (API Extractor)
- âœ… Consumo de API REST com retry automÃ¡tico
- âœ… Rate limiting para respeitar limites da API
- âœ… Exponential backoff em caso de falhas
- âœ… Logging detalhado de todas operaÃ§Ãµes

### Processamento (PySpark)
- âœ… ValidaÃ§Ãµes de qualidade de dados
- âœ… Schema enforcement
- âœ… DetecÃ§Ã£o de anomalias (preÃ§o e volume)
- âœ… TransformaÃ§Ãµes complexas e derivaÃ§Ãµes
- âœ… Particionamento otimizado

### Carga (Snowflake Loader)
- âœ… **Incremental Load**: Staging + MERGE (Type 2 SCD / UPSERT)
- âœ… **Stage + Merge**: write_pandas para estÃ¡gio e MERGE para silver/gold
- âœ… **Versioning**: HistÃ³rico completo de mudanÃ§as (is_current, valid_from/valid_to)
- âœ… **Views/Materialized**: Views e MVs para performance analÃ­tica

### OrquestraÃ§Ã£o (Databricks Jobs)
- âœ… Jobs agendados no Databricks
- âœ… Logs e monitoramento nativos
- âœ… Retry automÃ¡tico configurÃ¡vel
- âœ… Notifications (webhooks/email) via Databricks
- âœ… One-click rerun no workspace

## ğŸ“ Estrutura do Projeto

```
enterprise-data-pipeline/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml              # ConfiguraÃ§Ãµes centralizadas
â”‚   â””â”€â”€ .env.example             # Template de variÃ¡veis de ambiente
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_extractor.py         # ExtraÃ§Ã£o de dados da API
â”‚   â”œâ”€â”€ spark_processor.py       # Processamento PySpark
â”‚   â”œâ”€â”€ snowflake_loader.py      # Carga no Snowflake
â”‚   â””â”€â”€ pipeline_orchestrator.py # Orquestrador principal
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ snowflake_models.sql     # Views e modelos SQL
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py         # Testes unitÃ¡rios
â”œâ”€â”€ logs/                        # Logs de execuÃ§Ã£o
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ InstalaÃ§Ã£o e Setup

### 1. Clonar o RepositÃ³rio

```bash
git clone <repository-url>
cd enterprise-data-pipeline
```

### 2. Criar Ambiente Virtual

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configurar VariÃ¡veis de Ambiente

```bash
cp config/.env.example config/.env
# Editar .env com suas credenciais
```

### 5. Configurar Databricks

1. Criar workspace no Databricks
2. Criar cluster com PySpark 3.5+
3. Obter token de acesso
4. Configurar no `.env`

### 6. Configurar Snowflake (External Browser Auth)

```bash
# VariÃ¡veis no .env
SNOWFLAKE_ACCOUNT=seu-account-id
SNOWFLAKE_USER=seu-usuario
SNOWFLAKE_AUTHENTICATOR=externalbrowser
SNOWFLAKE_WAREHOUSE=SNOWFLAKE_LEARNING_WH
SNOWFLAKE_DATABASE=CRYPTO_DB
SNOWFLAKE_SCHEMA=PUBLIC
SNOWFLAKE_ROLE=ACCOUNTADMIN
```

1. Criar conta Snowflake (trial) e warehouse `SNOWFLAKE_LEARNING_WH`
2. Usar authenticator `externalbrowser` (nÃ£o armazena senha)
3. Testar conexÃ£o:
```bash
python test_snowflake_connection.py
```

### 7. (Opcional) Agendar no Databricks Jobs

1. No workspace, crie um Job
2. Tipo: Python script
3. Script: `src/pipeline_orchestrator.py`
4. Cluster: usar cluster existente
5. Schedule: defina o cron desejado
6. Defina variÃ¡veis/env via Secrets ou cluster env vars

## ğŸ® Como Usar

### ExecuÃ§Ã£o Manual (Standalone)

```bash
# Executar pipeline completo
cd src
python pipeline_orchestrator.py
```

### ExecuÃ§Ã£o via Databricks Jobs

- Crie um Job apontando para `src/pipeline_orchestrator.py`
- Configure parÃ¢metros/vars no Job ou via Secrets
- Agende o cron diretamente no Job

### ExecuÃ§Ã£o de Componentes Individuais

```bash
# Apenas extraÃ§Ã£o
python src/api_extractor.py

# Apenas processamento
python src/spark_processor.py

# Apenas carga
python src/snowflake_loader.py
```

## ğŸ“Š ValidaÃ§Ãµes e Qualidade de Dados

### Regras de ValidaÃ§Ã£o

1. **Campos ObrigatÃ³rios**: coin_id, symbol, current_price nÃ£o podem ser nulos
2. **Range de PreÃ§os**: 0 < price < 1,000,000
3. **Valores Positivos**: market_cap e volume devem ser > 0
4. **DetecÃ§Ã£o de Anomalias**: 
   - MudanÃ§a de preÃ§o > 50% em 24h
   - Volume 3x acima do desvio padrÃ£o

### MÃ©tricas de Qualidade

- Data Completeness Score
- Validation Success Rate
- Anomaly Detection Count
- Processing Time per Record

## ğŸ” Consultas SQL de Exemplo

### Ver Estado Atual do Mercado

```sql
SELECT * FROM v_current_market_state
ORDER BY market_cap_rank
LIMIT 10;
```

### Top Movers (24h)

```sql
SELECT * FROM v_top_movers_24h;
```

### HistÃ³rico de ExecuÃ§Ã£o do Pipeline

```sql
SELECT * FROM v_pipeline_execution_history
ORDER BY run_date DESC
LIMIT 20;
```

### Anomalias Detectadas

```sql
SELECT 
    symbol,
    name,
    current_price,
    price_change_percentage_24h,
    is_price_anomaly,
    is_volume_spike
FROM v_current_market_state
WHERE is_price_anomaly = TRUE OR is_volume_spike = TRUE;
```

## ğŸ¯ Features AvanÃ§adas

### 1. Incremental Load (Type 2 SCD)

```python
# MantÃ©m histÃ³rico completo de mudanÃ§as
# Campos: is_current, valid_from, valid_to
# Permite anÃ¡lise temporal
```

### 2. Schema Evolution

```python
# Suporta adiÃ§Ã£o de novas colunas
# NÃ£o quebra pipelines existentes
# Versionamento automÃ¡tico
```

### 3. Logging Estruturado

```python
# Logs em JSON com contexto completo
# Rastreamento de run_id
# MÃ©tricas de performance
```

### 4. Retry Logic

```python
# Exponential backoff
# Configurable retry attempts
# Circuit breaker pattern
```

## ğŸ“ˆ MÃ©tricas e Monitoramento

### Dashboard de MÃ©tricas

- Records Extracted
- Records Processed
- Records Loaded
- Data Quality Score
- Execution Time
- Error Rate

### Alertas Configurados

- Pipeline failure
- Data quality below threshold
- SLA breach
- Anomaly spike

## ğŸ§ª Testes

```bash
# Executar testes unitÃ¡rios
pytest tests/

# Com coverage
pytest --cov=src tests/

# Apenas testes especÃ­ficos
pytest tests/test_api_extractor.py
```

## ğŸ“ DocumentaÃ§Ã£o TÃ©cnica

### API Extractor

- **Rate Limit**: 50 requests/minuto
- **Timeout**: 30 segundos
- **Retry**: 3 tentativas com backoff

### Spark Processor

- **Partitions**: 200 (configurÃ¡vel)
- **Adaptive Query**: Habilitado
- **Memory**: 4GB driver, 8GB executors

### Snowflake Loader

- **Staging + MERGE**: write_pandas para stage e MERGE para silver/gold
- **Incremental**: Type 2 SCD (is_current, valid_from/valid_to)
- **Batch**: Carregamento em lote via stage tables
- **Views/MVs**: Views analÃ­ticas e materialized views

## ğŸš€ PrÃ³ximos Passos / Melhorias

- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar streaming com Kafka
- [ ] Dashboard em tempo real (Streamlit/Dash)
- [ ] Machine Learning para previsÃ£o de preÃ§os
- [ ] Data lineage com OpenLineage
- [ ] Testes de integraÃ§Ã£o end-to-end
- [ ] CI/CD com GitHub Actions
- [ ] ContainerizaÃ§Ã£o com Docker
- [ ] Deployment em Kubernetes

## ğŸ“š Tecnologias Utilizadas

| Categoria | Tecnologia | VersÃ£o |
|-----------|-----------|--------|
| Linguagem | Python | 3.9+ |
| Processing | PySpark | 3.5+ |
| Orchestration | Databricks Jobs | - |
| Warehouse | Snowflake | Enterprise |
| Compute | Databricks | Runtime 13.3+ |
| API | CoinGecko | v3 |

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT. Veja `LICENSE` para mais informaÃ§Ãµes.

## ğŸ‘¤ Autor

**Eric M.**

- LinkedIn: [seu-linkedin](https://linkedin.com/in/seu-perfil)
- Email: seu-email@example.com
- Portfolio: [seu-portfolio](https://seu-portfolio.com)

## ğŸ™ Agradecimentos

- CoinGecko pela API pÃºblica gratuita
- Comunidade Databricks e Snowflake
- Apache Airflow community

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela!
