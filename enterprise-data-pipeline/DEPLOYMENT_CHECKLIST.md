# ‚úÖ Deployment Checklist

Checklist completo para deploy em produ√ß√£o do Enterprise Data Pipeline.

## üìã Pr√©-Deployment

### Ambiente Local

- [ ] **Virtual environment criado e ativado**
  ```bash
  python -m venv venv
  source venv/bin/activate  # ou venv\Scripts\activate
  ```

- [ ] **Depend√™ncias instaladas**
  ```bash
  pip install -r requirements.txt
  ```

- [ ] **Testes passando localmente**
  ```bash
  pytest tests/ -v
  ```

- [ ] **Imports funcionando**
  ```bash
  python -c "from src.extractors.coingecko_extractor import CryptoExtractor; print('OK')"
  ```

- [ ] **Arquivo .env configurado**
  ```bash
  cp config/.env.example .env
  # Editar .env com credenciais reais
  ```

---

## üóÑÔ∏è Snowflake Setup

- [ ] **Conta Snowflake ativa**
  - [ ] Trial ou conta paga
  - [ ] Warehouse criado (COMPUTE_WH)

- [ ] **Database criado**
  ```sql
  CREATE DATABASE CRYPTO_DATA_PROD;
  ```

- [ ] **Schema criado**
  ```sql
  USE DATABASE CRYPTO_DATA_PROD;
  CREATE SCHEMA IF NOT EXISTS PUBLIC;
  ```

- [ ] **Tabelas criadas**
  ```sql
  -- Executar todo o conte√∫do de sql/snowflake_models.sql
  ```

- [ ] **Permiss√µes configuradas**
  ```sql
  GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE SYSADMIN;
  GRANT USAGE ON DATABASE CRYPTO_DATA_PROD TO ROLE SYSADMIN;
  GRANT ALL ON SCHEMA CRYPTO_DATA_PROD.PUBLIC TO ROLE SYSADMIN;
  ```

- [ ] **Conex√£o testada**
  ```bash
  python test_snowflake_connection.py
  ```

- [ ] **Query teste executada**
  ```sql
  SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();
  SHOW TABLES;
  ```

---

## ‚òÅÔ∏è Databricks Setup

### Workspace

- [ ] **Conta Databricks criada**
  - [ ] Community Edition OU
  - [ ] Trial/Paid account

- [ ] **Workspace acess√≠vel**
  - [ ] Login funcionando
  - [ ] Interface carregando

### Cluster

- [ ] **Cluster criado**
  - [ ] Nome: `crypto-pipeline-cluster`
  - [ ] Runtime: 13.3 LTS ou superior
  - [ ] Node Type: Standard_DS3_v2 ou similar
  - [ ] Workers: 2-4 (auto-scaling)

- [ ] **Bibliotecas instaladas**
  - [ ] `snowflake-connector-python[pandas]`
  - [ ] `pyyaml`
  - [ ] `tenacity`
  - [ ] `python-dotenv`

- [ ] **Cluster iniciado**
  - [ ] Status: Running
  - [ ] Spark UI acess√≠vel

### Reposit√≥rio

- [ ] **GitHub Repo conectado**
  - [ ] Workspace ‚Üí Repos ‚Üí Add Repo
  - [ ] URL do GitHub configurada
  - [ ] Branch: main

- [ ] **Arquivos sincronizados**
  - [ ] `notebooks/` vis√≠vel
  - [ ] `src/` vis√≠vel
  - [ ] `config/` vis√≠vel

- [ ] **Paths corrigidos nos notebooks**
  ```python
  # Em cada notebook, atualizar:
  sys.path.append("/Workspace/Repos/SEU-USERNAME/enterprise-data-pipeline/src")
  ```

### Secrets

- [ ] **Secret scope criado**
  ```python
  dbutils.secrets.createScope(scope="snowflake")
  ```

- [ ] **Secrets adicionados**
  - [ ] `account`
  - [ ] `user`
  - [ ] `password` (se n√£o usar externalbrowser)
  - [ ] `warehouse`
  - [ ] `database`
  - [ ] `schema`

- [ ] **Secrets testados**
  ```python
  # Em notebook:
  account = dbutils.secrets.get(scope="snowflake", key="account")
  print(f"Account: {account[:3]}...")  # Mostra apenas primeiros 3 chars
  ```

### DBFS

- [ ] **Path Bronze criado**
  ```python
  dbutils.fs.mkdirs("dbfs:/mnt/data/bronze/crypto/")
  ```

- [ ] **Permiss√µes verificadas**
  ```python
  dbutils.fs.ls("dbfs:/mnt/data/bronze/")
  ```

### Notebooks

- [ ] **01_extraction.py testado**
  - [ ] Anexado ao cluster
  - [ ] Par√¢metros configurados
  - [ ] Executado sem erros
  - [ ] Arquivo criado no DBFS

- [ ] **02_transformation.py testado**
  - [ ] Input path configurado
  - [ ] Executado sem erros
  - [ ] Views tempor√°rias criadas

- [ ] **03_loading.py testado**
  - [ ] Snowflake params configurados
  - [ ] Executado sem erros
  - [ ] Dados vis√≠veis no Snowflake

- [ ] **00_orchestrator.py testado**
  - [ ] Todos os notebooks chamados
  - [ ] Execu√ß√£o completa
  - [ ] Resultado JSON retornado

---

## üîÑ Databricks Job

### Configura√ß√£o

- [ ] **Job criado**
  - [ ] Workflows ‚Üí Create Job
  - [ ] Nome: `Crypto Data Pipeline`

- [ ] **Task configurado**
  - [ ] Task Name: `Orchestrate Pipeline`
  - [ ] Type: Notebook
  - [ ] Path: `/Repos/username/enterprise-data-pipeline/notebooks/00_orchestrator`

- [ ] **Cluster configurado**
  - [ ] Existing cluster OU
  - [ ] New job cluster (recomendado para prod)

- [ ] **Schedule configurado**
  - [ ] Cron expression correto (ex: `0 */6 * * *`)
  - [ ] Timezone correto
  - [ ] Status: Active

- [ ] **Alerts configurados**
  - [ ] Email on failure: SIM
  - [ ] Email: seu-email@exemplo.com

- [ ] **Advanced settings**
  - [ ] Timeout: 3600 seconds
  - [ ] Max Retries: 2
  - [ ] Retry Interval: 300 seconds

### Teste

- [ ] **Execu√ß√£o manual**
  - [ ] Click "Run Now"
  - [ ] Job iniciou

- [ ] **Monitoramento**
  - [ ] Runs tab acess√≠vel
  - [ ] Logs vis√≠veis
  - [ ] Output JSON correto

- [ ] **Verifica√ß√£o Snowflake**
  ```sql
  SELECT COUNT(*) FROM silver_crypto_clean;
  SELECT COUNT(*) FROM gold_crypto_metrics;
  SELECT * FROM pipeline_metadata ORDER BY execution_timestamp DESC LIMIT 1;
  ```

---

## üß™ Valida√ß√£o

### Dados

- [ ] **Bronze layer**
  - [ ] Arquivos JSON no DBFS
  - [ ] Timestamps corretos
  - [ ] Estrutura JSON v√°lida

- [ ] **Silver layer**
  - [ ] Registros no Snowflake
  - [ ] is_current corretamente configurado
  - [ ] valid_from/valid_to funcionando
  - [ ] Sem duplicatas

- [ ] **Gold layer**
  - [ ] M√©tricas agregadas corretas
  - [ ] Valores fazem sentido
  - [ ] Datas atualizadas

### Quality Checks

- [ ] **Completude**
  - [ ] Campos obrigat√≥rios preenchidos
  - [ ] Null percentage < 5%

- [ ] **Consist√™ncia**
  - [ ] Pre√ßos > 0
  - [ ] Market caps razo√°veis
  - [ ] Timestamps recentes

- [ ] **Duplicatas**
  - [ ] Nenhuma duplicata ativa (is_current=TRUE)
  - [ ] Hist√≥rico preservado

### Logs

- [ ] **Structured logs**
  - [ ] Logs em JSON
  - [ ] run_id presente
  - [ ] Events registrados

- [ ] **M√©tricas**
  - [ ] Records processed
  - [ ] Duration
  - [ ] Success/failure

---

## üìä Monitoramento

### Databricks

- [ ] **Job monitoring ativo**
  - [ ] Email alerts funcionando
  - [ ] Run history vis√≠vel

- [ ] **Cluster monitoring**
  - [ ] Spark UI acess√≠vel
  - [ ] M√©tricas de performance

### Snowflake

- [ ] **Query history**
  - [ ] Queries vis√≠veis
  - [ ] Performance aceit√°vel

- [ ] **Warehouse usage**
  - [ ] Credits usage razo√°vel
  - [ ] Auto-suspend configurado

### Alertas

- [ ] **Failure alerts**
  - [ ] Notifica√ß√µes funcionando
  - [ ] Email recebido em teste

- [ ] **Data quality alerts** (opcional)
  - [ ] Threshold configurado
  - [ ] Alertas testados

---

## üìù Documenta√ß√£o

- [ ] **README.md atualizado**
  - [ ] Badges corretos
  - [ ] Links funcionando

- [ ] **Docs organizados**
  - [ ] docs/ folder populado
  - [ ] Links internos funcionando

- [ ] **Runbook criado** (opcional)
  - [ ] Procedimentos operacionais
  - [ ] Troubleshooting

---

## üîí Seguran√ßa

- [ ] **Credenciais seguras**
  - [ ] N√£o commitadas no Git
  - [ ] Secrets no Databricks
  - [ ] .env no .gitignore

- [ ] **Permiss√µes m√≠nimas**
  - [ ] Apenas permiss√µes necess√°rias
  - [ ] Roles apropriados

- [ ] **Network security**
  - [ ] HTTPS para APIs
  - [ ] Conex√µes criptografadas

---

## üöÄ Go-Live

### Final Checks

- [ ] **Smoke test completo**
  - [ ] Executar pipeline manualmente
  - [ ] Verificar dados
  - [ ] Validar m√©tricas

- [ ] **Backup configs**
  - [ ] Commit no Git
  - [ ] Tag de release (v1.0.0)

- [ ] **Comunica√ß√£o**
  - [ ] Stakeholders notificados
  - [ ] Documenta√ß√£o compartilhada

### Ativa√ß√£o

- [ ] **Schedule ativado**
  - [ ] Job schedule: ON
  - [ ] Primeiro run agendado vis√≠vel

- [ ] **Monitoring ativo**
  - [ ] Alerts configurados
  - [ ] Dashboard pronto (se aplic√°vel)

---

## üìÖ P√≥s-Deploy

### Primeiras 24h

- [ ] **Monitorar primeira execu√ß√£o**
  - [ ] Job executou no hor√°rio
  - [ ] Sem erros
  - [ ] Dados corretos

- [ ] **Verificar custos**
  - [ ] Databricks usage
  - [ ] Snowflake credits

### Primeira Semana

- [ ] **Performance review**
  - [ ] Job duration aceit√°vel
  - [ ] Sem timeouts
  - [ ] Dados consistentes

- [ ] **Ajustes** (se necess√°rio)
  - [ ] Cluster size
  - [ ] Schedule frequency
  - [ ] Retention policies

### Primeiro M√™s

- [ ] **Retrospectiva**
  - [ ] Li√ß√µes aprendidas
  - [ ] Documenta√ß√£o atualizada
  - [ ] Melhorias identificadas

---

## üéØ Success Criteria

### Funcionalidade
‚úÖ Pipeline executa end-to-end sem erros  
‚úÖ Dados chegam no Snowflake  
‚úÖ Quality checks passando  
‚úÖ Logs estruturados gerados  

### Performance
‚úÖ Job completa em < 30 minutos  
‚úÖ Sem timeout errors  
‚úÖ Custos dentro do esperado  

### Confiabilidade
‚úÖ Retries funcionando  
‚úÖ Alertas notificando falhas  
‚úÖ Dados hist√≥ricos preservados  

---

## üìû Contatos de Suporte

**Databricks Support**: [Link do portal]  
**Snowflake Support**: [Link do portal]  
**Internal Team**: [Contato da equipe]  

---

**‚ú® Deployment completo! Pipeline em produ√ß√£o!**
