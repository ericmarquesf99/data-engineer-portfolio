# Databricks notebook source
# MAGIC %md
# MAGIC # ‚öôÔ∏è Transformation Notebook - Silver Layer
# MAGIC
# MAGIC **Input:** crypto_data_raw (do notebook 01_extraction)  
# MAGIC **Processing:** Limpeza, valida√ß√£o e transforma√ß√£o  
# MAGIC **Output:** crypto_data_silver (table tempor√°ria)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup

# COMMAND ----------

# MAGIC %pip install azure-identity azure-keyvault-secrets snowflake-connector-python[pandas] pyyaml tenacity requests

# COMMAND ----------

import sys
from datetime import datetime
import pandas as pd
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

# Add src to path
sys.path.append("/Workspace/Users/ericmarques1999@gmail.com/data-engineer-portfolio/enterprise-data-pipeline/src")

from transformers.spark_processor import SparkProcessor
from utils.logging_config import StructuredLogger
from utils.config_loader import load_config, get_snowflake_credentials_from_keyvault

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configure Azure Service Principal
# MAGIC
# MAGIC Credentials loaded from `config/credentials.yaml` file.

# COMMAND ----------

import os
import yaml

# Load credentials from config file
config_path = "/Workspace/Users/ericmarques1999@gmail.com/data-engineer-portfolio/enterprise-data-pipeline/config/credentials.yaml"

try:
    with open(config_path, 'r') as f:
        credentials = yaml.safe_load(f)
    
    os.environ['AZURE_TENANT_ID'] = credentials['azure']['tenant_id']
    os.environ['AZURE_CLIENT_ID'] = credentials['azure']['client_id']
    os.environ['AZURE_CLIENT_SECRET'] = credentials['azure']['client_secret']
    
    print("‚úÖ Azure Service Principal credentials loaded from config file")
except FileNotFoundError:
    print("‚ùå Error: config/credentials.yaml not found!")
    print("üí° Copy config/credentials.yaml.example to config/credentials.yaml and fill with your values")
    raise
except Exception as e:
    print(f"‚ùå Error loading credentials: {e}")
    raise

# COMMAND ----------

# Get parameters
dbutils.widgets.text("run_id", "", "Run ID")

run_id = dbutils.widgets.get("run_id")

logger = StructuredLogger("transformation")
logger.log_event("transformation_notebook_started", {"run_id": run_id})

# COMMAND ----------

# MAGIC %md
# MAGIC ## Retrieve Data from Bronze Table in Snowflake

# COMMAND ----------

# Retrieve Snowflake credentials from Azure Key Vault
try:
    snowflake_config = get_snowflake_credentials_from_keyvault("kv-crypto-pipeline")
    logger.log_event("snowflake_credentials_loaded", {"vault": "kv-crypto-pipeline"})
    print("‚úÖ Snowflake credentials retrieved from Key Vault")
except Exception as e:
    logger.log_event("keyvault_error", {"error": str(e)}, level="ERROR")
    print(f"‚ùå Error retrieving credentials: {e}")
    raise

# COMMAND ----------

import snowflake.connector
import json

# Connect to Snowflake using BRONZE schema
conn = snowflake.connector.connect(
    account=snowflake_config['account'],
    user=snowflake_config['user'],
    password=snowflake_config['password'],
    warehouse=snowflake_config['warehouse'],
    database=snowflake_config['database'],
    schema='BRONZE'
)
cur = conn.cursor()

# Ensure Bronze schema
cur.execute("USE SCHEMA BRONZE")

# Fetch data from CRYPTO_RAW table
logger.log_event("reading_bronze_table", {"table": "BRONZE.CRYPTO_RAW"})
print("üìä Reading data from BRONZE.CRYPTO_RAW table...")

cur.execute("""
    SELECT 
        id,
        payload,
        extracted_at,
        run_id,
        source_system,
        created_at
    FROM BRONZE.CRYPTO_RAW
    WHERE processed = FALSE
    ORDER BY created_at DESC
""")

# Convert results to list of dicts
bronze_data = []
for row in cur.fetchall():
    payload_json = json.loads(row[1]) if isinstance(row[1], str) else row[1]
    bronze_data.append(payload_json)

# cur.close()
# conn.close()

print(f"‚úÖ {len(bronze_data)} records retrieved from Bronze")

# Convert to Spark DataFrame
df_bronze_pandas = pd.json_normalize(bronze_data)
df_bronze = spark.createDataFrame(df_bronze_pandas)

print(f"\nüìä Available columns:")
df_bronze.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cleaning and Transformation

# COMMAND ----------

start_time = datetime.now()

try:
    # Load configuration
    config = load_config()
    
    # Initialize processor
    processor = SparkProcessor(config)
    
    logger.log_event("transforming_bronze_to_silver", {
        "input_records": df_bronze.count()
    })
    
    # Basic cleaning processing
    df_silver = df_bronze \
        .dropDuplicates() \
        .filter(F.col("id").isNotNull()) \
        .filter(F.col("current_price").isNotNull())
    
    # Add processing column
    df_silver = df_silver.withColumn(
        "processed_at",
        F.lit(datetime.now().isoformat())
    )
    
    # Convert numeric columns
    numeric_cols = ["current_price", "market_cap", "total_volume", "market_cap_rank"]
    for col in numeric_cols:
        if col in df_silver.columns:
            df_silver = df_silver.withColumn(col, F.col(col).cast(DoubleType()))
    
    silver_count = df_silver.count()
    
    # NOTE: Cache disabled - Databricks Community Edition does not support DataFrame caching
    # df_silver.cache()
    
    logger.log_event("silver_transformation_completed", {
        "output_records": silver_count
    })
    
    print(f"‚úÖ Silver Layer: {silver_count} records processed")
    print(f"\nüìä Data sample:")
    df_silver.select("id", "symbol", "current_price", "market_cap", "processed_at").limit(5).display()
    
    # Load Silver to Snowflake
    logger.log_event("loading_silver_to_snowflake", {"records": silver_count})
    
    # Use SILVER schema
    cur.execute("USE SCHEMA SILVER")
    
    # Collect data from Spark DataFrame
    silver_data = df_silver.collect()
    
    inserted_silver = 0
    for row in silver_data:
        # Insert each record into silver_crypto_clean table
        cur.execute("""
            INSERT INTO silver_crypto_clean (
                coin_id, symbol, name, current_price, market_cap, 
                market_cap_rank, total_volume, price_change_percentage_24h,
                updated_at, run_id, is_current
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, TRUE)
        """, (
            row['id'],
            row['symbol'] if 'symbol' in row else None,
            row['name'] if 'name' in row else None,
            float(row['current_price']) if 'current_price' in row and row['current_price'] else None,
            float(row['market_cap']) if 'market_cap' in row and row['market_cap'] else None,
            int(row['market_cap_rank']) if 'market_cap_rank' in row and row['market_cap_rank'] else None,
            float(row['total_volume']) if 'total_volume' in row and row['total_volume'] else None,
            float(row['price_change_percentage_24h']) if 'price_change_percentage_24h' in row and row['price_change_percentage_24h'] else None,
            row['processed_at'],
            run_id
        ))
        inserted_silver += 1
    
    conn.commit()
    
    logger.log_event("silver_loaded_to_snowflake", {"records": inserted_silver})
    print(f"‚ùÑÔ∏è  Snowflake Silver: {inserted_silver} records inserted")
    
except Exception as e:
    logger.log_event("transformation_error", {"error": str(e)}, level="ERROR")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## Create Gold Layer (Aggregations)

# COMMAND ----------

try:
    # Create aggregations by market cap category
    logger.log_event("creating_gold_aggregations", {})
    
    # Add market cap categorization
    df_categorized = df_silver.withColumn(
        "market_cap_category",
        F.when(F.col("market_cap") >= 10000000000, "LARGE_CAP")
         .when(F.col("market_cap") >= 1000000000, "MID_CAP")
         .otherwise("SMALL_CAP")
    )
    
    # Agregar por categoria
    df_gold = df_categorized \
        .groupBy("market_cap_category") \
        .agg(
            F.count("*").alias("num_coins"),
            F.sum("market_cap").alias("total_market_cap"),
            F.avg("market_cap").alias("avg_market_cap"),
            F.sum("total_volume").alias("total_volume"),
            F.avg("current_price").alias("avg_price"),
            F.avg("price_change_percentage_24h").alias("avg_price_change_24h")
        )
    
    gold_count = df_gold.count()
    
    # NOTE: Cache disabled - Databricks Community Edition does not support DataFrame caching
    # df_gold.cache()
    
    logger.log_event("gold_aggregation_completed", {
        "categories": gold_count
    })
    
    print(f"‚úÖ Gold Layer: {gold_count} categorias agregadas")
    print(f"\nüìä Amostra das agrega√ß√µes:")
    df_gold.display()
    
    # Load Gold to Snowflake
    logger.log_event("loading_gold_to_snowflake", {"categories": gold_count})
    
    # Use GOLD schema
    cur.execute("USE SCHEMA GOLD")
    
    # Collect aggregated data
    gold_data = df_gold.collect()
    
    inserted_gold = 0
    for row in gold_data:
        cur.execute("""
            INSERT INTO gold_crypto_metrics (
                metric_date, market_cap_category, num_coins, total_market_cap,
                avg_market_cap, total_volume, avg_price, avg_price_change_24h,
                created_at, run_id
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            datetime.now().date(),
            row['market_cap_category'],
            int(row['num_coins']),
            float(row['total_market_cap']) if row['total_market_cap'] else None,
            float(row['avg_market_cap']) if row['avg_market_cap'] else None,
            float(row['total_volume']) if row['total_volume'] else None,
            float(row['avg_price']) if row['avg_price'] else None,
            float(row['avg_price_change_24h']) if row['avg_price_change_24h'] else None,
            datetime.now(),
            run_id
        ))
        inserted_gold += 1
    
    conn.commit()
    
    logger.log_event("gold_loaded_to_snowflake", {"categories": inserted_gold})
    print(f"‚ùÑÔ∏è  Snowflake Gold: {inserted_gold} categorias inseridas")
    
except Exception as e:
    logger.log_event("gold_aggregation_error", {"error": str(e)}, level="ERROR")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## Finalizar

# COMMAND ----------

# Fechar conex√£o Snowflake
cur.close()
conn.close()

print("‚úÖ Conex√£o Snowflake fechada")

# Create temporary views (optional, for compatibility)
df_silver.createOrReplaceTempView("crypto_data_silver")
df_gold.createOrReplaceTempView("crypto_data_gold")

logger.log_event("temp_views_created", {
    "silver_view": "crypto_data_silver",
    "gold_view": "crypto_data_gold"
})

print("‚úÖ Views tempor√°rias criadas (opcional)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resultado Final

# COMMAND ----------

end_time = datetime.now()
duration = (end_time - start_time).total_seconds()

result = {
    "status": "success",
    "silver_records": inserted_silver,
    "gold_categories": inserted_gold,
    "duration_seconds": duration
}

logger.log_event("transformation_completed", result)

print(f"\n‚úÖ Transforma√ß√£o completa!")
print(f"   Bronze ‚Üí Silver: {inserted_silver} registros")
print(f"   Silver ‚Üí Gold: {inserted_gold} categorias")
print(f"   ‚è±Ô∏è  Dura√ß√£o: {duration:.2f}s")

# COMMAND ----------

# Retornar resultado
dbutils.notebook.exit(json.dumps(result))
